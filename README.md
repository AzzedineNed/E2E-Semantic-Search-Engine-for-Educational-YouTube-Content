Hereâ€™s a clean and professional `README.md` adapted to your new project, reflecting the expanded multi-channel capability:

---

# ğŸ¯ End-to-End Semantic Search Engine for Educational YouTube Content

Semantic search engine to explore multiple educational YouTube channels using sentence embeddings and vector similarity. Supports natural language queries across a curated set of tech and programming-focused channels.

---

## ğŸ“Œ Features

* ğŸ” **Semantic Search** over content from multiple educational YouTube channels.
* âš™ï¸ **ETL Pipeline** for automated fetching, processing, and embedding of video metadata.
* ğŸš€ **FastAPI Backend**, containerized with Docker and deployable to **Google Cloud Run**.
* ğŸŒ **Gradio UI** integration possible (via Hugging Face Spaces or local).
* â˜ï¸ **CI/CD** with GitHub Actions and Google Cloud Build for automatic updates.

---

## ğŸ“š Supported Channels

Currently indexed YouTube channels:

* `freecodecamp`
* `3blue1brown`
* `firstprinciples`
* `jeffheaton`
* `krishnaik`
* `netninja`
* `techworldnana`
* `theconstructsim`
* `yousuckatprogramming`

Each channel has its own searchable index.

---

## ğŸ—ï¸ System Architecture

![System Architecture](https://github.com/AzzedineNed/E2E-Semantic-Search-Engine-for-Educational-YouTube-Content/blob/main/end-to-end-semantic-search-for-freecodecamp-videos.png)

---

## ğŸ§  Semantic Search Logic

Uses the [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model from `sentence-transformers` to embed video metadata and search queries.

* Comparison is done via **Manhattan distance** (L1 norm).
* Embedding indexes are stored in `.parquet` files inside `app/data/`.

Example:

```bash
curl -X POST "http://localhost:8080/search/netninja?query=routes%20basics"
```

---

## ğŸ“‚ Project Structure

```
E2E-Semantic-Search-Engine-for-Educational-YouTube-Content
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ data/                        # Parquet indexes for each channel
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # FastAPI entrypoint
â”‚   â””â”€â”€ search_function.py           # Search logic
â”œâ”€â”€ data_pipeline
â”‚   â”œâ”€â”€ Data_Pipeline.py             # Main pipeline script
â”‚   â”œâ”€â”€ ETL.py                       # ETL logic (fetch, clean, embed)
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ data-pipeline.yml        # Scheduled GitHub Actions workflow
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## ğŸ”„ Data Pipeline

Automated pipeline using GitHub Actions:

* ğŸ•“ Scheduled to run **every Saturday at 4:00 AM** or manually.
* ğŸ§¬ Fetches video metadata using **YouTube Data API v3**.
* ğŸ§¼ Cleans and transforms video titles, descriptions, and transcripts.
* ğŸ¤– Generates sentence embeddings using `sentence-transformers`.
* ğŸ’¾ Saves `.parquet` indexes for each channel in `app/data/`.

> Only the `app/` directory is included in the Docker image.

---

## ğŸ” Secrets & Configuration

To enable CI/CD and pipeline execution, set the following GitHub repository secrets:

| Secret Name             | Description                                         |
| ----------------------- | --------------------------------------------------- |
| `YT_API_KEY`            | YouTube Data API v3 key                             |
| `PERSONAL_ACCESS_TOKEN` | GitHub Personal Access Token (for workflow commits) |

â¡ï¸ Set these under: GitHub â†’ `Settings` â†’ `Secrets and variables` â†’ `Actions`

---

## ğŸš€ Deployment

* **App:** FastAPI application running on port `8080`.
* **Containerized:** With Docker.
* **Cloud Ready:** Deployable to **Google Cloud Run**.
* **CI/CD:** GitHub Actions + Google Cloud Build for automated deployments on push.

---

## ğŸ³ Docker

### Build the container

```bash
docker build -t semantic-search-app .
```

### Run locally

```bash
docker run -p 8080:8080 semantic-search-app
```

---

## ğŸ”§ Local Development

### Run FastAPI app

```bash
cd app
uvicorn main:app --host 0.0.0.0 --port 8080
```

### Run the ETL pipeline

```bash
cd data_pipeline
python Data_Pipeline.py
```

---

## ğŸ” Test the API

Example POST request:

```bash
curl "http://localhost:8080/search/netninja?query=routes%20basics"
```
---
